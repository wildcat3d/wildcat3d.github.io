<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="WildCAT3D: Appearance-Aware Multi-View Diffusion in the Wild. We present a framework for generating novel views of scenes learned from diverse 2D scene image data captured in the wild.">
    <meta name="keywords" content="WildCAT3D, Novel View Synthesis, Multi-view Diffusion, Appearance Control">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>WildCAT3D: Appearance-Aware Multi-View Diffusion in the Wild</title>

    <meta property="og:title" content="WildCAT3D: Appearance-Aware Multi-View Diffusion in the Wild" />
    <meta property="og:description"
        content="We present a framework for generating novel views of scenes learned from diverse 2D scene image data captured in the wild." />

    <meta property="twitter:card" content="summary" />
    <meta property="twitter:title" content="WildCAT3D: Appearance-Aware Multi-View Diffusion in the Wild" />
    <meta property="twitter:description"
        content="We present a framework for generating novel views of scenes learned from diverse 2D scene image data captured in the wild." />

    <!-- MathJax library -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
        async></script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.4/css/bulma.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./styles.css">
</head>

<body>
    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-2 publication-title">WildCAT3D: Appearance-Aware Multi-View Diffusion in the
                            Wild</h1>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="http://morrisalp.github.io">Morris Alper</a><sup>1*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://d-novotny.github.io/">David Novotny</a><sup>2</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.fkokkinos.com">Filippos Kokkinos</a><sup>2</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.hadarelor.com">Hadar Averbuch-Elor</a><sup>3</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://tmonnier.com">Tom Monnier</a><sup>2</sup>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors publication-authors-affiliation">
                            <span class="author-block"><sup>1</sup>Tel Aviv University</span> &nbsp;&nbsp;
                            <span class="author-block"><sup>2</sup>Meta AI</span> &nbsp;&nbsp;
                            <span class="author-block"><sup>3</sup>Cornell University</span>
                        </div>

                        <div class="is-size-6 publication-authors-note">
                            <span>*Work done while interning at Meta AI</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <span class="link-block">
                                    <a href="#" class="external-link button is-normal is-rounded is-primary">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>

                                <span class="link-block">
                                    <a href="./videos/video_results_viewer.html"
                                        class="external-link button is-normal is-rounded interactive-button">
                                        <span class="icon">
                                            <i class="fas fa-play"></i>
                                        </span>
                                        <span>Video Results</span>
                                    </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Teaser Video Section -->
    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <video class="teaser-video" autoplay muted loop playsinline>
                    <source src="./resources/teaser_video.mp4" type="video/mp4">
                </video>
                <h2 class="subtitle has-text-centered teaser-subtitle">
                    WildCAT3D generates consistent novel views from single images while enabling appearance control
                </h2>
            </div>
        </div>
    </section>

    <!-- Abstract Section -->
    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-full-width has-text-centered">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Despite recent advances in sparse novel view synthesis (NVS) applied to object-centric
                            scenes, scene-level NVS remains a challenge. A central issue is the lack of available clean
                            multi-view training data, beyond manually curated datasets with limited diversity, camera
                            variation, or licensing issues. On the other hand, an abundance of diverse and
                            permissively-licensed data exists in the wild, consisting of scenes with varying appearances
                            (illuminations, transient occlusions, etc.) from sources such as tourist photos.
                            To this end, we present <strong>WildCAT3D</strong>, a framework for generating novel views
                            of scenes learned from diverse 2D scene image data captured in the wild. We unlock training
                            on these data sources by explicitly modeling global appearance conditions in images,
                            extending the state-of-the-art multi-view diffusion paradigm to learn from scene views of
                            varying appearances. Our trained model generalizes to new scenes at inference time, enabling
                            the generation of multiple consistent novel views.
                            WildCAT3D provides state-of-the-art results on single-view NVS in object- and scene-level
                            settings, while training on strictly less data sources than prior methods. Additionally, it
                            enables novel applications by providing global appearance control during generation.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Method Section -->
    <section class="section section-alt-bg">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-full-width has-text-centered">
                    <h2 class="title is-3">Method</h2>
                    <div class="content has-text-justified">
                        <p>
                            Our key insight is that inconsistent data can be leveraged during multi-view diffusion
                            training to learn consistent generation, by specifically decoupling content and appearance
                            when denoising novel views. Starting from the multi-view diffusion framework, we propose to
                            explicitly integrate a feed-forward appearance model that captures the appearance properties
                            of input views.
                        </p>
                        <p>
                            We add an appearance encoding branch to produce low-dimensional appearance embeddings used
                            as conditioning signals for the multi-view diffusion model. Additionally, we employ a warp
                            conditioning mechanism that warps pixels from the source view following the target viewpoint
                            using depth information, helping resolve scale ambiguity inherent to single-view NVS.
                        </p>
                    </div>
                    <div class="content has-text-centered">
                        <img class="method-diagram" src="./images/system.jpg" alt="WildCAT3D Method Overview">
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Results Section -->
    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-full-width has-text-centered">
                    <h2 class="title is-3">Results</h2>
                    <div class="content has-text-justified">
                        <p>
                            WildCAT3D demonstrates superior performance on standard novel view synthesis benchmarks
                            while
                            training on diverse in-the-wild data. Our method excels at generating consistent novel views
                            from single input images using various camera trajectories including lateral turns,
                            zoom-outs,
                            and NeRF-like circular paths. We achieve state-of-the-art results on both object-centric and
                            scene-level novel view synthesis tasks.
                        </p>
                    </div>

                    <div class="results-grid">
                        <div class="result-item">
                            <video autoplay muted loop playsinline>
                                <source src="./resources/result_1.mp4" type="video/mp4">
                            </video>
                            <div class="content">
                                <h4>Circular Camera Trajectories</h4>
                                <p>Novel views generated with NeRF-like circular camera paths from single input images
                                </p>
                            </div>
                        </div>
                        <div class="result-item">
                            <video autoplay muted loop playsinline>
                                <source src="./resources/result_2.mp4" type="video/mp4">
                            </video>
                            <div class="content">
                                <h4>Lateral Camera Movements</h4>
                                <p>Consistent view synthesis with lateral turn trajectories</p>
                            </div>
                        </div>
                        <div class="result-item">
                            <video autoplay muted loop playsinline>
                                <source src="./resources/result_3.mp4" type="video/mp4">
                            </video>
                            <div class="content">
                                <h4>Zoom-Out Trajectories</h4>
                                <p>High-quality novel views with zoom-out camera motions</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Applications Section -->
    <section class="section">
        <div class="container is-max-desktop">
            <div class="appearance-control">
                <h2 class="title is-3 has-text-centered">Applications</h2>
                <div class="content has-text-centered">
                    <p class="appearance-control-text">
                        Our explicit modeling of appearance enables novel applications such as appearance-controlled
                        generation
                        using external conditioning images and interpolation between views of differing appearances.
                    </p>

                    <!-- Appearance Control -->
                    <div class="application-demo">
                        <h4 class="title is-4">Appearance-Controlled Generation</h4>
                        <div class="demo-row">
                            <div class="demo-item">
                                <img src="./videos/applications/appearance_conditioned_gen/5_gt.png" alt="Input view">
                                <p>Input View</p>
                            </div>
                            <div class="demo-item">
                                <img src="./videos/applications/appearance_conditioned_gen/5_appearance.png"
                                    alt="Appearance condition">
                                <p>Appearance Condition</p>
                            </div>
                            <div class="demo-item">
                                <img src="./videos/applications/appearance_conditioned_gen/5_gen.gif"
                                    alt="Generated result">
                                <p>Generated Novel Views</p>
                            </div>
                        </div>
                    </div>

                    <!-- Interpolation -->
                    <div class="application-demo">
                        <h4 class="title is-4">In-the-Wild Interpolation</h4>
                        <div class="demo-row">
                            <div class="demo-item">
                                <img src="./videos/applications/interpolation/5_start.png" alt="Start view">
                                <p>Start View</p>
                            </div>
                            <div class="demo-item">
                                <img src="./videos/applications/interpolation/5_interp.gif"
                                    alt="Generated interpolation sequence">
                                <p>Generated Interpolation Sequence</p>
                            </div>
                            <div class="demo-item">
                                <img src="./videos/applications/interpolation/5_end.png" alt="End view">
                                <p>End View</p>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Comparison Section -->
    <section class="section">
        <div class="container is-max-desktop">
            <div class="comparison-section">
                <h2 class="title is-3 has-text-centered">Comparison with Prior Work</h2>
                <div class="content has-text-justified">
                    <p>
                        WildCAT3D significantly outperforms the previous SOTA MegaScenes NVS model (MS NVS) at
                        generating consistent and high-quality novel view sequences from single images. Our method
                        achieves superior performance while training on unfiltered data in-the-wild, unlike the
                        aggressive filtering
                        used by prior methods.
                    </p>
                </div>
                <div class="comparison-videos">
                    <div class="comparison-item">
                        <h4 class="title is-5">MS NVS (Previous SOTA)</h4>
                        <video autoplay muted loop playsinline>
                            <source src="./resources/ms_nvs_comparison.mp4" type="video/mp4">
                        </video>
                    </div>
                    <div class="comparison-item">
                        <h4 class="title is-5">WildCAT3D (Ours)</h4>
                        <video autoplay muted loop playsinline>
                            <source src="./resources/wildcat3d_comparison.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- BibTeX Section -->
    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title is-3">BibTeX</h2>
            <pre class="bibtex-code"><code>@article{alper2025wildcat3d,
  title={WildCAT3D: Appearance-Aware Multi-View Diffusion in the Wild},
  author={Alper, Morris and Novotny, David and Kokkinos, Filippos and Averbuch-Elor, Hadar and Monnier, Tom},
  journal={arXiv preprint},
  year={2025}
}</code></pre>
        </div>
    </section>

    <!-- Acknowledgements Section -->
    <section class="section section-alt-bg">
        <div class="container is-max-desktop content">
            <h2 class="title is-3">Acknowledgments</h2>
            <p>
                This work was sponsored by Meta AI. We thank Kush Jain and Keren Ganon for providing helpful feedback.
            </p>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            This webpage template is adapted from <a
                                href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>,
                            under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0
                                License</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>